/**
 * Text Utilities for Speech
 * =========================
 * Utility functions to prepare text for speech synthesis.
 * Strips markdown and other formatting that shouldn't be spoken.
 *
 * FROZEN: Feb 6, 2026
 *
 * This module contains:
 *
 * 1. stripMarkdownForSpeech(text)  â€” Strips markdown from bot messages before TTS.
 *    Used by the middleware to clean bot responses so the ponyfill doesn't speak
 *    asterisks, hashtags, link URLs, etc.
 *
 * 2. BargeInController  â€” Monitors microphone volume via Web Audio API and triggers
 *    a callback when the user speaks above a threshold for a sustained duration.
 *    The callback calls speechSynthesis.cancel() on the ponyfill's own instance
 *    to stop TTS audio immediately.
 *
 *    STATUS: âš ï¸  EXPERIMENTAL â€” The BargeInController initializes its own AudioContext
 *    and getUserMedia stream. Browser restrictions may prevent AudioContext from
 *    resuming without a user gesture. The controller has a late-init fallback that
 *    retries initialization when startMonitoring() is called.
 *
 *    KNOWN LIMITATION: The barge-in cancel calls ponyfill speechSynthesis.cancel()
 *    directly which stops audio, but Web Chat's internal speaking state may not
 *    update (no dispatch into the store from middleware to avoid re-entrant crashes).
 *    This means the UI "speaking" indicator may stay on briefly after cancel.
 *
 * 3. createSpeechMiddleware(options) â€” Redux-style middleware for botframework-webchat's
 *    createStore(). Observes Web Chat actions to:
 *      - Track speech activity state (idle/listening/processing/speaking)
 *      - Start/stop barge-in monitoring when bot speaks
 *      - Strip markdown from incoming bot messages before TTS
 *      - Set inputHint on messages that lack one
 *
 *    IMPORTANT: This middleware must NOT call dispatch() â€” doing so re-enters the
 *    Web Chat store during action processing and causes "Render error" crashes.
 *    All side-effects (TTS cancel, barge-in) use external callbacks instead.
 *
 * Settings Wiring Summary (see also VoiceSettingsPanel.tsx and README.md):
 * â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 * â”‚ Setting               â”‚ Where it takes effect                                   â”‚ Status   â”‚
 * â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 * â”‚ locale                â”‚ Ponyfill credentials (server) + Web Chat locale prop    â”‚ âœ… Works â”‚
 * â”‚ voice                 â”‚ Ponyfill speechSynthesisVoiceName (hook)                â”‚ âœ… Works â”‚
 * â”‚ speechRate            â”‚ PatchedUtterance wrapper in hook (rate property)         â”‚ âœ… Works â”‚
 * â”‚ speechPitch           â”‚ PatchedUtterance wrapper in hook (pitch property)        â”‚ âœ… Works â”‚
 * â”‚ continuousRecognition â”‚ styleOptions.speechRecognitionContinuous (component)     â”‚ âœ… Works â”‚
 * â”‚ autoStartMic          â”‚ Ctrl+M keyboard event after connect (component)          â”‚ âœ… Works â”‚
 * â”‚ autoResumeListening   â”‚ Ctrl+M after 'speaking'â†’'idle' transition (component)   â”‚ âœ… Works â”‚
 * â”‚ bargeInEnabled        â”‚ BargeInController.setConfig() (component)                â”‚ âš ï¸ Exp.  â”‚
 * â”‚ bargeInSensitivity    â”‚ BargeInController.setConfig() (component)                â”‚ âš ï¸ Exp.  â”‚
 * â”‚ interimResults        â”‚ NOT wired â€” Web Chat DictateComposer controls internally â”‚ âŒ N/A   â”‚
 * â”‚ silenceTimeoutMs      â”‚ NOT wired â€” Azure Speech SDK recognizer controls this    â”‚ âŒ N/A   â”‚
 * â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 */

import type { DirectLineSpeechSettings, PonyfillSettings } from '../components/VoiceSettingsPanel';

/**
 * Strip markdown syntax from text for TTS
 * Removes asterisks, hashtags, links, code blocks, etc.
 */
export function stripMarkdownForSpeech(text: string): string {
  if (!text) return '';
  
  let cleaned = text;
  
  // Remove code blocks (```...```)
  cleaned = cleaned.replace(/```[\s\S]*?```/g, ' code block ');
  
  // Remove inline code (`...`)
  cleaned = cleaned.replace(/`([^`]+)`/g, '$1');
  
  // Remove images ![alt](url)
  cleaned = cleaned.replace(/!\[([^\]]*)\]\([^)]+\)/g, '$1');
  
  // Convert links [text](url) to just the text
  cleaned = cleaned.replace(/\[([^\]]+)\]\([^)]+\)/g, '$1');
  
  // Remove bold/italic markers (**, __, *, _)
  cleaned = cleaned.replace(/\*\*\*([^*]+)\*\*\*/g, '$1'); // ***bold italic***
  cleaned = cleaned.replace(/\*\*([^*]+)\*\*/g, '$1');     // **bold**
  cleaned = cleaned.replace(/\*([^*]+)\*/g, '$1');         // *italic*
  cleaned = cleaned.replace(/___([^_]+)___/g, '$1');       // ___bold italic___
  cleaned = cleaned.replace(/__([^_]+)__/g, '$1');         // __bold__
  cleaned = cleaned.replace(/_([^_]+)_/g, '$1');           // _italic_
  
  // Remove strikethrough (~~text~~)
  cleaned = cleaned.replace(/~~([^~]+)~~/g, '$1');
  
  // Remove headers (# ## ### etc.)
  cleaned = cleaned.replace(/^#{1,6}\s+/gm, '');
  
  // Remove horizontal rules (---, ***, ___)
  cleaned = cleaned.replace(/^[-*_]{3,}\s*$/gm, '');
  
  // Remove blockquotes (> text)
  cleaned = cleaned.replace(/^>\s+/gm, '');
  
  // Remove unordered list markers (- * +)
  cleaned = cleaned.replace(/^[\s]*[-*+]\s+/gm, '');
  
  // Remove ordered list markers (1. 2. etc.)
  cleaned = cleaned.replace(/^[\s]*\d+\.\s+/gm, '');
  
  // Remove HTML tags
  cleaned = cleaned.replace(/<[^>]+>/g, '');
  
  // Remove extra whitespace
  cleaned = cleaned.replace(/\s+/g, ' ');
  
  // Trim
  cleaned = cleaned.trim();
  
  return cleaned;
}

/**
 * Speech activity types that Web Chat dispatches
 */
export type SpeechActivityType = 
  | 'idle'
  | 'listening'
  | 'processing'
  | 'speaking';

/**
 * Barge-in sensitivity presets
 */
export const BARGE_IN_PRESETS = {
  low: { detectionDelayMs: 500, volumeThreshold: 0.5 },
  medium: { detectionDelayMs: 200, volumeThreshold: 0.3 },
  high: { detectionDelayMs: 50, volumeThreshold: 0.15 },
};

/**
 * Barge-in controller for stopping TTS when user speaks.
 *
 * FROZEN: Feb 6, 2026
 * STATUS: âš ï¸ EXPERIMENTAL
 *
 * How it works:
 * 1. initialize() â€” Creates an AudioContext + getUserMedia stream, connects
 *    to an AnalyserNode for real-time volume monitoring.
 * 2. startMonitoring(onBargeIn) â€” Polls volume every 50ms. If normalized
 *    volume exceeds the threshold for longer than detectionDelayMs, triggers
 *    onBargeIn callback. Has a late-init fallback if initialize() failed.
 * 3. stopMonitoring() â€” Stops the polling interval.
 * 4. destroy() â€” Stops monitoring, closes stream & AudioContext.
 *
 * The onBargeIn callback (set by the component) calls:
 *   speechSynthesisRef.current.cancel()  â€” stops ponyfill TTS audio
 *   onSpeechActivity('idle')             â€” updates UI state
 *
 * Known issues:
 * - AudioContext may start suspended (browser restriction). The controller
 *   tries to resume it, but some browsers block this without user gesture.
 * - The ponyfill's speechSynthesis.cancel() stops audio but Web Chat's
 *   internal speaking state is not updated (we cannot dispatch from middleware).
 * - Volume threshold may need tuning for different microphones/environments.
 */
export class BargeInController {
  private audioContext: AudioContext | null = null;
  private analyser: AnalyserNode | null = null;
  private mediaStream: MediaStream | null = null;
  private volumeCheckInterval: number | null = null;
  private sensitivity: 'low' | 'medium' | 'high' = 'medium';
  private enabled: boolean = true;
  private onBargeIn: (() => void) | null = null;
  private isMonitoring: boolean = false;

  async initialize(): Promise<void> {
    try {
      this.audioContext = new AudioContext();
      
      // AudioContext often starts suspended â€” must resume it
      if (this.audioContext.state === 'suspended') {
        console.log('ðŸŽ¤ AudioContext suspended, resuming...');
        await this.audioContext.resume();
      }
      console.log(`ðŸŽ¤ AudioContext state: ${this.audioContext.state}`);
      
      this.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log(`ðŸŽ¤ Got microphone stream with ${this.mediaStream.getAudioTracks().length} track(s)`);
      
      const source = this.audioContext.createMediaStreamSource(this.mediaStream);
      this.analyser = this.audioContext.createAnalyser();
      this.analyser.fftSize = 256;
      source.connect(this.analyser);
      
      console.log('âœ… Barge-in controller initialized (analyser ready)');
    } catch (error) {
      console.error('âŒ Could not initialize barge-in audio monitoring:', error);
      this.analyser = null;
    }
  }

  setConfig(enabled: boolean, sensitivity: 'low' | 'medium' | 'high'): void {
    this.enabled = enabled;
    this.sensitivity = sensitivity;
    console.log(`âš™ï¸ Barge-in config: enabled=${enabled}, sensitivity=${sensitivity}`);
  }

  startMonitoring(onBargeIn: () => void): void {
    if (!this.enabled) {
      console.log('â­ï¸ Barge-in: skipping monitoring (disabled)');
      return;
    }
    if (!this.analyser) {
      console.warn('âš ï¸ Barge-in: analyser is null â€” mic not initialized. Trying to re-initialize...');
      // Try to initialize now (user has already interacted with page)
      this.initialize().then(() => {
        if (this.analyser) {
          console.log('âœ… Barge-in: late initialization succeeded, starting monitoring');
          this.startMonitoring(onBargeIn);
        } else {
          console.error('âŒ Barge-in: late initialization failed, analyser still null');
        }
      });
      return;
    }
    if (this.isMonitoring) {
      console.log('â­ï¸ Barge-in: already monitoring');
      return;
    }
    
    // Resume AudioContext if suspended (can happen without user gesture)
    if (this.audioContext?.state === 'suspended') {
      console.log('ðŸŽ¤ Resuming suspended AudioContext...');
      this.audioContext.resume();
    }
    
    this.isMonitoring = true;
    this.onBargeIn = onBargeIn;
    
    const dataArray = new Uint8Array(this.analyser.frequencyBinCount);
    const preset = BARGE_IN_PRESETS[this.sensitivity];
    
    let speechDetectedTime: number | null = null;
    let bargeInTriggered = false;
    let logCount = 0;
    
    console.log(`ðŸŽ¤ Barge-in: monitoring STARTED (sensitivity=${this.sensitivity}, threshold=${preset.volumeThreshold}, delay=${preset.detectionDelayMs}ms)`);
    
    this.volumeCheckInterval = window.setInterval(() => {
      if (!this.analyser || bargeInTriggered) return;
      
      this.analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume
      const average = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
      const normalizedVolume = average / 255;
      
      // Log volume periodically so user can see it's working
      logCount++;
      if (logCount % 20 === 0) { // Every ~1 second (50ms * 20)
        console.log(`ðŸŽ¤ Barge-in: volume=${normalizedVolume.toFixed(3)}, threshold=${preset.volumeThreshold}, speaking=${speechDetectedTime ? 'yes' : 'no'}`);
      }
      
      if (normalizedVolume > preset.volumeThreshold) {
        if (!speechDetectedTime) {
          speechDetectedTime = Date.now();
          console.log(`ðŸŽ¤ Barge-in: speech detected! volume=${normalizedVolume.toFixed(3)} > ${preset.volumeThreshold}`);
        } else if (Date.now() - speechDetectedTime > preset.detectionDelayMs) {
          // User has been speaking long enough - trigger barge-in
          bargeInTriggered = true;
          console.log(`ðŸ›‘ Barge-in TRIGGERED! Sustained speech for ${Date.now() - speechDetectedTime}ms`);
          this.onBargeIn?.();
          this.stopMonitoring();
        }
      } else {
        speechDetectedTime = null;
      }
    }, 50);
  }

  stopMonitoring(): void {
    if (this.volumeCheckInterval) {
      clearInterval(this.volumeCheckInterval);
      this.volumeCheckInterval = null;
    }
    this.isMonitoring = false;
  }

  destroy(): void {
    this.stopMonitoring();
    
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
    }
    
    if (this.audioContext) {
      this.audioContext.close();
    }
  }
}

/**
 * Options for creating the speech middleware
 */
export interface SpeechMiddlewareOptions {
  onSpeechActivity?: (activity: SpeechActivityType) => void;
  bargeInController?: BargeInController;
  settings?: DirectLineSpeechSettings | PonyfillSettings;
  onStopSpeaking?: () => void;
}

/**
 * Create a middleware for Web Chat that:
 * 1. Strips markdown from bot messages before TTS
 * 2. Tracks speech activity for the code panel
 * 3. Handles barge-in when enabled
 */
export function createSpeechMiddleware(options: SpeechMiddlewareOptions = {}) {
  const { onSpeechActivity, bargeInController, onStopSpeaking } = options;
  
  return () => (next: (action: any) => any) => (action: any) => {
    // Track speech-related actions
    switch (action.type) {
      case 'WEB_CHAT/START_DICTATE':
        onSpeechActivity?.('listening');
        break;
        
      case 'WEB_CHAT/STOP_DICTATE':
        onSpeechActivity?.('idle');
        break;
        
      case 'WEB_CHAT/SET_DICTATE_STATE':
        if (action.payload?.dictateState === 1) { // STARTING
          onSpeechActivity?.('listening');
          bargeInController?.stopMonitoring();
          // User started talking â€” cancel TTS via ponyfill
          onStopSpeaking?.();
        } else if (action.payload?.dictateState === 3) { // STOPPING
          onSpeechActivity?.('processing');
        } else if (action.payload?.dictateState === 0) { // IDLE
          onSpeechActivity?.('idle');
        }
        break;
        
      case 'WEB_CHAT/START_SPEAKING_ACTIVITY':
        onSpeechActivity?.('speaking');
        // Start monitoring for barge-in while bot is speaking
        bargeInController?.startMonitoring(() => {
          console.log('ðŸ›‘ Barge-in triggered â€” cancelling ponyfill TTS');
          onStopSpeaking?.();
          onSpeechActivity?.('idle');
        });
        break;
        
      case 'WEB_CHAT/STOP_SPEAKING_ACTIVITY':
        bargeInController?.stopMonitoring();
        onSpeechActivity?.('idle');
        break;
        
      case 'DIRECT_LINE/POST_ACTIVITY':
        if (action.payload?.activity?.type === 'message') {
          onSpeechActivity?.('processing');
        }
        break;
    }
    
    // Intercept incoming activities to strip markdown for TTS
    if (action.type === 'DIRECT_LINE/INCOMING_ACTIVITY') {
      const activity = action.payload?.activity;
      
      if (activity?.type === 'message' && activity?.text) {
        // If there's no explicit speak property, create one from cleaned text
        if (!activity.speak) {
          activity.speak = stripMarkdownForSpeech(activity.text);
        } else {
          // Also clean the speak property in case it has markdown
          activity.speak = stripMarkdownForSpeech(activity.speak);
        }
        
        // If no inputHint is set, default to acceptingInput
        // This tells Web Chat the bot is ready for more input
        // Note: 'expectingInput' would re-open the mic after speech
        if (!activity.inputHint) {
          activity.inputHint = 'acceptingInput';
        }
      }
    }
    
    return next(action);
  };
}

/**
 * Legacy function for backwards compatibility
 */
export function createMarkdownStripMiddleware(onSpeechActivity?: (activity: SpeechActivityType) => void) {
  return createSpeechMiddleware({ onSpeechActivity });
}

export default stripMarkdownForSpeech;
